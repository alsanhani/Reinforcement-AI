{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stable-baselines3.readthedocs.io/en/master/guide/rl.html\n",
    "# https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.19.5)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.1.4)\n",
      "Requirement already satisfied: gym>=0.17 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.18.3)\n",
      "Requirement already satisfied: tensorboard>=2.2.0; extra == \"extra\" in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.6.0)\n",
      "Requirement already satisfied: psutil; extra == \"extra\" in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.7.0)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.2.9)\n",
      "Requirement already satisfied: opencv-python; extra == \"extra\" in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.1.1.26)\n",
      "Requirement already satisfied: pillow; extra == \"extra\" in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (7.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alwaleed\\appdata\\roaming\\python\\python37\\site-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.7.4.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2020.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.0)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.15)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.13.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\alwaleed\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (49.2.0.post20200714)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.6.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\alwaleed\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.37.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.18.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.21.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\alwaleed\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.40.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from atari-py~=0.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\alwaleed\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = \"ALE/Assault-v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:18.0\n",
      "Episode:2 Score:26.0\n",
      "Episode:3 Score:18.0\n",
      "Episode:4 Score:10.0\n",
      "Episode:5 Score:21.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding The Environment\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0-push cart to left, 1-push cart to the right\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7297070e-01,  2.9441005e+38, -3.1801918e-01, -1.6179726e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train an RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1206 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015156938 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.0178      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.67        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 46          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 699          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031722905 |\n",
      "|    clip_fraction        | 0.0442       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.668       |\n",
      "|    explained_variance   | 0.0784       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.6         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0142      |\n",
      "|    value_loss           | 42.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013027688 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 53.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 637          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064507537 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.614       |\n",
      "|    explained_variance   | 0.166        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.5         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0121      |\n",
      "|    value_loss           | 69.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009860496 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 74.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007187961 |\n",
      "|    clip_fraction        | 0.039       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    value_loss           | 55.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 619         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004251983 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0042     |\n",
      "|    value_loss           | 44.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 613          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049141953 |\n",
      "|    clip_fraction        | 0.0647       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.577       |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.9         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    value_loss           | 58.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 611         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005130495 |\n",
      "|    clip_fraction        | 0.0682      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.756       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 49.3        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    value_loss           | 62.9        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x26f67a42608>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('PPO_model', env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alwaleed\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=5, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info [{'TimeLimit.truncated': True, 'terminal_observation': array([ 0.5648082 , -0.02823569, -0.07839766,  0.3231096 ])}]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        print('info', info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Viewing Logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Adding a callback to the training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1)\n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('Training', 'Saved Models', 'best_model')\n",
    "model = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Changing Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch=[dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, policy_kwargs={'net_arch': net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Using an Alternate Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(dqn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(dqn_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
